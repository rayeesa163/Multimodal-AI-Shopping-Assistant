 Project: Multimodal AI Shopping Assistant
Duration: Jun 2025 – Jul 2025
Live Demo: YouTube Link
GitHub: rayeesa163/Multimodal-AI-Shopping-Assistant

Description:
A cloud-deployed AI web app that takes user input (text query + real-time emotion via webcam) and generates personalized product recommendations instantly. This project demonstrates seamless integration of multiple AI modalities, real-time inference, and interactive UI.

Key Features:

Captures user emotion using DeepFace

Understands user intent using BERT (Sentence Transformers)

Recommends products by combining emotion + intent through Cosine Similarity

Interactive real-time interface built with Streamlit

Tech Stack: Python, OpenCV, PyTorch, Streamlit

Skills Demonstrated:

Multimodal AI integration (vision + NLP)

Real-time AI inference and recommendation systems

Frontend UI design for AI apps

Cloud-ready deployment and interactive web apps

Why it fits the minimal AI generation concept:

Takes live input → AI generates meaningful output instantly

Fully functional demo deployed in real-time

Clean and user-friendly interface for practical interaction

1. Clone the repo:
```bash
git clone https://github.com/your-username/Multimodal-AI-Assistant.git
cd Multimodal-AI-Assistant
  
